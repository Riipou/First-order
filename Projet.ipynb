{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK7497ogsBmbfuyMJlIawr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riipou/First-order/blob/main/Projet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "bl83xkta8l4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import csv"
      ],
      "metadata": {
        "id": "N-hIFm_oAlSB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chargement et pré-traitement des données"
      ],
      "metadata": {
        "id": "SYh8dg7z8O3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(y):\n",
        "    new_y = []\n",
        "    for label in y:\n",
        "        label_vector = np.zeros(20)\n",
        "        label_vector[label - 1] = 1\n",
        "        new_y.append(label_vector)\n",
        "    y = np.array(new_y)\n",
        "    return y\n",
        "\n",
        "\n",
        "def shuffle(X, y):\n",
        "    index = np.arange(np.shape(X)[1])\n",
        "    np.random.shuffle(index)\n",
        "    X = X[:, index]\n",
        "    y = y.T[:, index]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def split_data(X, y, split_value=0.8):\n",
        "    m = X.shape[1]\n",
        "    m = int(m * split_value)\n",
        "    X_train = X[:, :m]\n",
        "    y_train = y[:m, :]\n",
        "    X_test = X[:, m:]\n",
        "    y_test = y[m:, :]\n",
        "    return X_train, y_train, X_test, y_test\n"
      ],
      "metadata": {
        "id": "MDPtzI948q-e"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le fichier .mat\n",
        "nom_fichier_mat = 'data_doc.mat'\n",
        "donnees = loadmat(nom_fichier_mat)\n",
        "\n",
        "# Xts combien de fois le mot dans le texte\n",
        "if 'Xts' in donnees:\n",
        "    Xts = donnees['Xts']\n",
        "else:\n",
        "    print(\"La variable 'Xts' n'a pas été trouvée dans le fichier.\")\n",
        "\n",
        "# id texte avec classe\n",
        "if 'yts' in donnees:\n",
        "    yts = donnees['yts']\n",
        "else:\n",
        "    print(\"La variable 'yts' n'a pas été trouvée dans le fichier.\")\n",
        "\n",
        "# Xvr combien de fois le mot dans le texte\n",
        "if 'Xvr' in donnees:\n",
        "    Xvr = donnees['Xvr']\n",
        "else:\n",
        "    print(\"La variable 'Xvr' n'a pas été trouvée dans le fichier.\")\n",
        "\n",
        "yts = one_hot_encoding(yts)\n",
        "Xts, yts = shuffle(Xts, yts)\n",
        "# X_train, y_train, X_test, y_test = split_data(Xts, yts)"
      ],
      "metadata": {
        "id": "O0WvebYQ8Obr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Descente du gradient"
      ],
      "metadata": {
        "id": "9P5g_HL68dYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y, y_pred):\n",
        "    n = y.shape[1]\n",
        "    loss = (np.linalg.norm(y - y_pred, 'fro') ** 2) / n\n",
        "    return loss\n",
        "\n",
        "def sigmoid(X, a):\n",
        "    return 1 / (1 + np.exp(-a * X))\n",
        "\n",
        "def sigmoid_derivate(X, a):\n",
        "    return a * sigmoid(X, a) * (1 - sigmoid(X, a))\n",
        "\n",
        "def relu(x, a):\n",
        "    return np.maximum(0, a * x)\n",
        "\n",
        "def initialization_random(m, p):\n",
        "    W = np.random.rand(m, p) - 0.5\n",
        "    b = np.random.rand(p) - 0.5\n",
        "    return W, b\n",
        "\n",
        "def initialization_xavier(m, p):\n",
        "    W = np.random.randn(m, p) * np.sqrt(2 / (m + p))\n",
        "    b = np.zeros((p,))\n",
        "    return W, b\n",
        "\n",
        "def initialization_lecun(m, p):\n",
        "    W = np.random.randn(m, p) * np.sqrt(1 / m)\n",
        "    b = np.zeros((p,))\n",
        "    return W, b\n",
        "\n",
        "def grad_W(W, B, a, X, y, n):\n",
        "    return X @ ((1 / n) * np.multiply(sigmoid(W.T @ X + B, a) - y, sigmoid_derivate(W.T @ X + B, a))).T\n",
        "\n",
        "def grad_B(W, B, a, X, y, n):\n",
        "    v1 = np.ones((n, 1))\n",
        "    return ((1 / n) * np.multiply(sigmoid(W.T @ X + B, a) - y, sigmoid_derivate(W.T @ X + B, a))) @ v1\n",
        "\n",
        "def update_W(alpha, W, B, a, X, y):\n",
        "    loss = loss_function(y, sigmoid(W.T @ X + B, a))\n",
        "    alpha *= 1.5\n",
        "    W2 = W - alpha * grad_W(W, B, a, X, y, n)\n",
        "    while loss_function(y, sigmoid(W2.T @ X + B, a)) > loss:\n",
        "        alpha /= 2\n",
        "        W2 = W - alpha * grad_W(W, B, a, X, y, n)\n",
        "    return W2, alpha\n",
        "\n",
        "def update_B(alpha, W, B, a, X, y):\n",
        "    loss = loss_function(y, sigmoid(W.T @ X + B, a))\n",
        "    alpha *= 1.5\n",
        "    b = B[:, 1]\n",
        "    b = b.reshape(-1, 1)\n",
        "    b2 = b - alpha * grad_B(W, B, a, X, y, n)\n",
        "    B2 = np.tile(b2.reshape(-1), (n, 1)).T\n",
        "    while loss_function(y, sigmoid(W.T @ X + B2, a)) > loss:\n",
        "        alpha /= 2\n",
        "        b2 = b - alpha * grad_B(W, B, a, X, y, n)\n",
        "        B2 = np.tile(b2.reshape(-1), (n, 1)).T\n",
        "    return B2, alpha\n",
        "\n",
        "\n",
        "def linear_model(m, n, p, max_iterations, a, precision, X, y):\n",
        "\n",
        "    # Initialisation random pour les poids (76,987)\n",
        "    # W, b = initialization_wavier(m, p)\n",
        "\n",
        "    # Initialisation de Xavier/Glorot pour les poids (73,513)\n",
        "    # W, b = initialization_xavier(m, p)\n",
        "\n",
        "    # Initialisation de Lecun pour les poids (77,388)\n",
        "    W, b = initialization_lecun(m, p)\n",
        "\n",
        "    B = np.tile(b, (n, 1)).T\n",
        "    alpha_W = 0.1 * np.linalg.norm(W, 'fro') / np.linalg.norm(grad_W(W, B, a, X, y, n), 'fro')\n",
        "    alpha_B = 0.1 * np.linalg.norm(B, 'fro') / np.linalg.norm(grad_B(W, B, a, X, y, n), 'fro')\n",
        "    for _ in range(max_iterations):\n",
        "        loss_prev = loss_function(y, sigmoid(W.T @ X + B, 1))\n",
        "        W, alpha_W = update_W(alpha_W, W, B, a, X, y)\n",
        "        B, alpha_B = update_B(alpha_B, W, B, a, X, y)\n",
        "        loss = loss_function(y, sigmoid(W.T @ X + B, 1))\n",
        "        print(loss)\n",
        "        if abs(loss - loss_prev) < precision:\n",
        "            print(\"break\")\n",
        "            break\n",
        "    return W, b\n",
        "\n"
      ],
      "metadata": {
        "id": "AJFQyc6K8zcm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descente de gradient"
      ],
      "metadata": {
        "id": "VanXa1rM8ZM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = Xts.shape\n",
        "p, _ = yts.shape\n",
        "\n",
        "num_iterations=100000\n",
        "a = 0.05\n",
        "precision = 1e-5\n",
        "\n",
        "\n",
        "W, b = linear_model(m, n, p, num_iterations, a, precision, Xts, yts)"
      ],
      "metadata": {
        "id": "Ur52FKOD8X4h"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation des données"
      ],
      "metadata": {
        "id": "qJI9qCsn7-qc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJCkdiVX79_P"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cD63PHxohXrQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création validation"
      ],
      "metadata": {
        "id": "Ffv4AvSIhYEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_one_hot_encoding(y):\n",
        "    y = np.argmax(y, axis=0)+1\n",
        "    return y\n",
        "\n",
        "def test(X, W, b):\n",
        "    _, n = X.shape\n",
        "    B = np.tile(b, (n, 1)).T\n",
        "    y = sigmoid(W.T @ X + B, 1)\n",
        "    y = reverse_one_hot_encoding(y)\n",
        "    ids = np.arange(1, len(y)+1)\n",
        "    data = np.column_stack((ids, 1+100 * y))\n",
        "    nom_fichier = \"test2.csv\"\n",
        "\n",
        "    header = [\"id\",\"class\"]\n",
        "    with open(nom_fichier, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "    return 0"
      ],
      "metadata": {
        "id": "dJVFpZAeEaJC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(Xvr, W, b)"
      ],
      "metadata": {
        "id": "RV6Rmf9-D7xe",
        "outputId": "308e4f97-109a-4707-e5b2-482256427bf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-d0f8aeaf1f40>:7: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-a * X))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}
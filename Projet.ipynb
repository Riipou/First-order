{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK7497ogsBmbfuyMJlIawr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riipou/First-order/blob/main/Projet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "bl83xkta8l4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import csv"
      ],
      "metadata": {
        "id": "N-hIFm_oAlSB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chargement et pré-traitement des données"
      ],
      "metadata": {
        "id": "SYh8dg7z8O3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(y):\n",
        "    new_y = []\n",
        "    for label in y:\n",
        "        label_vector = np.zeros(20)\n",
        "        label_vector[label - 1] = 1\n",
        "        new_y.append(label_vector)\n",
        "    y = np.array(new_y)\n",
        "    return y\n",
        "\n",
        "\n",
        "def shuffle(X, y):\n",
        "    index = np.arange(np.shape(X)[1])\n",
        "    np.random.shuffle(index)\n",
        "    X = X[:, index]\n",
        "    y = y.T[:, index]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def split_data(X, y, split_value=0.8):\n",
        "    m = X.shape[1]\n",
        "    m = int(m * split_value)\n",
        "    X_train = X[:, :m]\n",
        "    y_train = y[:m, :]\n",
        "    X_test = X[:, m:]\n",
        "    y_test = y[m:, :]\n",
        "    return X_train, y_train, X_test, y_test\n"
      ],
      "metadata": {
        "id": "MDPtzI948q-e"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le fichier .mat\n",
        "nom_fichier_mat = 'data_doc.mat'\n",
        "donnees = loadmat(nom_fichier_mat)\n",
        "\n",
        "# Xts combien de fois le mot dans le texte\n",
        "if 'Xts' in donnees:\n",
        "    Xts = donnees['Xts']\n",
        "else:\n",
        "    print(\"La variable 'Xts' n'a pas été trouvée dans le fichier.\")\n",
        "\n",
        "# id texte avec classe\n",
        "if 'yts' in donnees:\n",
        "    yts = donnees['yts']\n",
        "else:\n",
        "    print(\"La variable 'yts' n'a pas été trouvée dans le fichier.\")\n",
        "\n",
        "# Xvr combien de fois le mot dans le texte\n",
        "if 'Xvr' in donnees:\n",
        "    Xvr = donnees['Xvr']\n",
        "else:\n",
        "    print(\"La variable 'Xvr' n'a pas été trouvée dans le fichier.\")\n",
        "\n",
        "yts = one_hot_encoding(yts)\n",
        "Xts, yts = shuffle(Xts, yts)\n",
        "# X_train, y_train, X_test, y_test = split_data(Xts, yts)"
      ],
      "metadata": {
        "id": "O0WvebYQ8Obr"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Descente du gradient"
      ],
      "metadata": {
        "id": "9P5g_HL68dYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y, y_pred):\n",
        "    n = y.shape[1]\n",
        "    loss = (np.linalg.norm(y - y_pred, 'fro') ** 2) / n\n",
        "    return loss\n",
        "\n",
        "def sigmoid(X, a):\n",
        "    return 1 / (1 + np.exp(-a * X))\n",
        "\n",
        "def sigmoid_derivate(X, a):\n",
        "    return a * sigmoid(X, a) * (1 - sigmoid(X, a))\n",
        "\n",
        "def relu(x, a):\n",
        "    return np.maximum(0, a * x)\n",
        "\n",
        "def initialization_random(m, p):\n",
        "    W = np.random.rand(m, p) - 0.5\n",
        "    b = np.random.rand(p) - 0.5\n",
        "    return W, b\n",
        "\n",
        "def initialization_xavier(m, p):\n",
        "    W = np.random.randn(m, p) * np.sqrt(2 / (m + p))\n",
        "    b = np.zeros((p,))\n",
        "    return W, b\n",
        "\n",
        "def initialization_lecun(m, p):\n",
        "    W = np.random.randn(m, p) * np.sqrt(1 / m)\n",
        "    b = np.zeros((p,))\n",
        "    return W, b\n",
        "\n",
        "def grad_W(W, B, a, X, y, n):\n",
        "    return X @ ((1 / n) * np.multiply(sigmoid(W.T @ X + B, a) - y, sigmoid_derivate(W.T @ X + B, a))).T\n",
        "\n",
        "def grad_B(W, B, a, X, y, n):\n",
        "    v1 = np.ones((n, 1))\n",
        "    return ((1 / n) * np.multiply(sigmoid(W.T @ X + B, a) - y, sigmoid_derivate(W.T @ X + B, a))) @ v1\n",
        "\n",
        "def update_W(alpha, W, B, a, X, y):\n",
        "    loss = loss_function(y, sigmoid(W.T @ X + B, a))\n",
        "    alpha *= 1.5\n",
        "    W2 = W - alpha * grad_W(W, B, a, X, y, n)\n",
        "    while loss_function(y, sigmoid(W2.T @ X + B, a)) > loss:\n",
        "        alpha /= 2\n",
        "        W2 = W - alpha * grad_W(W, B, a, X, y, n)\n",
        "    return W2, alpha\n",
        "\n",
        "def update_B(alpha, W, B, a, X, y):\n",
        "    loss = loss_function(y, sigmoid(W.T @ X + B, a))\n",
        "    alpha *= 1.5\n",
        "    b = B[:, 1]\n",
        "    b = b.reshape(-1, 1)\n",
        "    b2 = b - alpha * grad_B(W, B, a, X, y, n)\n",
        "    B2 = np.tile(b2.reshape(-1), (n, 1)).T\n",
        "    while loss_function(y, sigmoid(W.T @ X + B2, a)) > loss:\n",
        "        alpha /= 2\n",
        "        b2 = b - alpha * grad_B(W, B, a, X, y, n)\n",
        "        B2 = np.tile(b2.reshape(-1), (n, 1)).T\n",
        "    return B2, alpha\n",
        "\n",
        "\n",
        "def linear_model(m, n, p, max_iterations, a, precision, X, y):\n",
        "\n",
        "    # Initialisation random pour les poids (76,987)\n",
        "    # W, b = initialization_wavier(m, p)\n",
        "\n",
        "    # Initialisation de Xavier/Glorot pour les poids (73,513)\n",
        "    # W, b = initialization_xavier(m, p)\n",
        "\n",
        "    # Initialisation de Lecun pour les poids (77,388)\n",
        "    W, b = initialization_lecun(m, p)\n",
        "\n",
        "    B = np.tile(b, (n, 1)).T\n",
        "    alpha_W = 0.1 * np.linalg.norm(W, 'fro') / np.linalg.norm(grad_W(W, B, a, X, y, n), 'fro')\n",
        "    alpha_B = 0.1 * np.linalg.norm(B, 'fro') / np.linalg.norm(grad_B(W, B, a, X, y, n), 'fro')\n",
        "    for _ in range(max_iterations):\n",
        "        loss_prev = loss_function(y, sigmoid(W.T @ X + B, 1))\n",
        "        W, alpha_W = update_W(alpha_W, W, B, a, X, y)\n",
        "        B, alpha_B = update_B(alpha_B, W, B, a, X, y)\n",
        "        loss = loss_function(y, sigmoid(W.T @ X + B, 1))\n",
        "        print(loss)\n",
        "        if abs(loss - loss_prev) < precision:\n",
        "            print(\"break\")\n",
        "            break\n",
        "    return W, b\n",
        "\n"
      ],
      "metadata": {
        "id": "AJFQyc6K8zcm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descente de gradient"
      ],
      "metadata": {
        "id": "VanXa1rM8ZM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = Xts.shape\n",
        "p, _ = yts.shape\n",
        "\n",
        "num_iterations=100000\n",
        "a = 0.05\n",
        "precision = 1e-5\n",
        "\n",
        "\n",
        "W, b = linear_model(m, n, p, num_iterations, a, precision, Xts, yts)"
      ],
      "metadata": {
        "id": "Ur52FKOD8X4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4fb15c-7c33-42f6-b165-a461f25b3b86"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.831677972104292\n",
            "2.8363168033789137\n",
            "2.110804191686314\n",
            "1.6493022510372604\n",
            "1.3803475597566905\n",
            "1.225792918882283\n",
            "1.1335174922456834\n",
            "1.0771798361000262\n",
            "1.042783524750171\n",
            "1.0214312268326529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-37600a3db8f4>:7: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-a * X))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0069197431640378\n",
            "0.995678071416539\n",
            "0.984350405302947\n",
            "0.970804096059198\n",
            "0.9503465667040029\n",
            "0.9243904744198863\n",
            "0.8889434396514099\n",
            "0.842502541917613\n",
            "0.7703020243523353\n",
            "0.6737501691984503\n",
            "0.5842818712830755\n",
            "0.5215079159960435\n",
            "0.48583904119267024\n",
            "0.4472965451169802\n",
            "0.45869477899502353\n",
            "0.43533420022842995\n",
            "0.4002165264767054\n",
            "0.39619950942114407\n",
            "0.33138824621340984\n",
            "0.29821984680058355\n",
            "0.24599778055056162\n",
            "0.25945943238488106\n",
            "0.23200010832342136\n",
            "0.24315280425555072\n",
            "0.20020479465901128\n",
            "0.2023653377874734\n",
            "0.18969269375187056\n",
            "0.17461470898090936\n",
            "0.164239522678413\n",
            "0.16553908987925922\n",
            "0.15500583801288983\n",
            "0.15969765656530432\n",
            "0.14984858272166793\n",
            "0.14282993311080613\n",
            "0.14207544820899864\n",
            "0.13772449652346266\n",
            "0.14112840838788396\n",
            "0.1372606701224287\n",
            "0.13354489313880993\n",
            "0.13124495960720292\n",
            "0.12988750549514466\n",
            "0.12684125257709578\n",
            "0.12666733534872288\n",
            "0.1242174479721127\n",
            "0.12446243752962766\n",
            "0.12231267135189103\n",
            "0.12394913214631943\n",
            "0.12001446004065335\n",
            "0.12094306746518\n",
            "0.11822686625780553\n",
            "0.11971198570004955\n",
            "0.11702931439407128\n",
            "0.11710472411274886\n",
            "0.11500836768214766\n",
            "0.11566159669908689\n",
            "0.11411428836819268\n",
            "0.11525647600509029\n",
            "0.11266117664674133\n",
            "0.11382942214692095\n",
            "0.111599273485883\n",
            "0.11311262674077943\n",
            "0.11050152408732473\n",
            "0.11017827289315\n",
            "0.10889855467837539\n",
            "0.10934273594357588\n",
            "0.10815716095808979\n",
            "0.1090964685932963\n",
            "0.1069771893732912\n",
            "0.10786191167447098\n",
            "0.10629320314134134\n",
            "0.10645163441198952\n",
            "0.10545307622224925\n",
            "0.10641943284614874\n",
            "0.10513291156005611\n",
            "0.1049153320071954\n",
            "0.10355384866328121\n",
            "0.10367662848683278\n",
            "0.10318844843335119\n",
            "0.10397105551073056\n",
            "0.1023425911494445\n",
            "0.10189714015074848\n",
            "0.1014988111895476\n",
            "0.10236933194645068\n",
            "0.10175895613142565\n",
            "0.101203785367744\n",
            "0.10111969394996836\n",
            "0.10104160714499942\n",
            "0.09989475444675515\n",
            "0.10018735459097108\n",
            "0.09954518188342804\n",
            "0.10009755893387542\n",
            "0.09936962936196037\n",
            "0.09883728375502372\n",
            "0.09856343110444842\n",
            "0.09863460415831349\n",
            "0.09843753050388876\n",
            "0.09871355268386393\n",
            "0.09762751012191766\n",
            "0.09809991399778177\n",
            "0.09746879790218019\n",
            "0.09805831401140544\n",
            "0.09752034747614245\n",
            "0.09701117622155384\n",
            "0.09682587828002838\n",
            "0.09778596013892019\n",
            "0.09754194589373849\n",
            "0.09700431730842786\n",
            "0.09579131251517795\n",
            "0.0962397858956439\n",
            "0.09652261427617116\n",
            "0.09670766778247625\n",
            "0.09609975606243627\n",
            "0.09669012720326729\n",
            "0.0959374487036562\n",
            "0.09521092823342325\n",
            "0.0949524519555529\n",
            "0.09497267113263023\n",
            "0.09492954035889704\n",
            "0.0951698168648499\n",
            "0.09527602106062862\n",
            "0.09519878856390965\n",
            "0.09420180034564772\n",
            "0.09466733837977831\n",
            "0.09413540925799048\n",
            "0.09471206075046071\n",
            "0.09426143030098558\n",
            "0.09360911461026761\n",
            "0.09334418350871027\n",
            "0.09341058321442085\n",
            "0.09336086621691504\n",
            "0.09348963371483883\n",
            "0.09239849981814441\n",
            "0.0929430012404708\n",
            "0.09308299592425005\n",
            "0.09317029035620374\n",
            "0.09256442640529614\n",
            "0.09170270213559618\n",
            "0.09133126152995834\n",
            "0.09223108958654845\n",
            "0.09186248211795327\n",
            "0.09172545424954169\n",
            "0.09152707139147773\n",
            "0.09143529492036916\n",
            "0.09032414171778459\n",
            "0.09093669061818975\n",
            "0.09005861874518915\n",
            "0.09078800383032919\n",
            "0.09017977923449894\n",
            "0.08963137328868115\n",
            "0.08926258786663582\n",
            "0.09011199735233588\n",
            "0.08866277124193181\n",
            "0.08878006260637679\n",
            "0.08892645120136632\n",
            "0.08933929567078291\n",
            "0.0884999609169735\n",
            "0.08909950413621506\n",
            "0.08850689088667384\n",
            "0.08902193122195975\n",
            "0.0886937534611352\n",
            "0.08801205163265796\n",
            "0.08769404759258942\n",
            "0.08769427558729553\n",
            "break\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation des données"
      ],
      "metadata": {
        "id": "qJI9qCsn7-qc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJCkdiVX79_P"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cD63PHxohXrQ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création validation"
      ],
      "metadata": {
        "id": "Ffv4AvSIhYEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_one_hot_encoding(y):\n",
        "    y = np.argmax(y, axis=0)+1\n",
        "    return y\n",
        "\n",
        "def test(X, W, b):\n",
        "    _, n = X.shape\n",
        "    B = np.tile(b, (n, 1)).T\n",
        "    y = sigmoid(W.T @ X + B, 1)\n",
        "    y = reverse_one_hot_encoding(y)\n",
        "    ids = np.arange(1, len(y)+1)\n",
        "    data = np.column_stack((ids, 1+100 * y))\n",
        "    nom_fichier = \"test2.csv\"\n",
        "\n",
        "    header = [\"id\",\"class\"]\n",
        "    with open(nom_fichier, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "    return 0"
      ],
      "metadata": {
        "id": "dJVFpZAeEaJC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(Xvr, W, b)"
      ],
      "metadata": {
        "id": "RV6Rmf9-D7xe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5f747f-3874-4a45-d031-944cc41d0319"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-37600a3db8f4>:7: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-a * X))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}